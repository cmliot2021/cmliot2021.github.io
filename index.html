<!DOCTYPE html>
<html lang="en">
  <head>
    <title>CML-IOT 2020</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link href="https://fonts.googleapis.com/css?family=Amatic+SC:400,700|Work+Sans:300,400,700" rel="stylesheet">
    <link rel="stylesheet" href="fonts/icomoon/style.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/magnific-popup.css">
    <link rel="stylesheet" href="css/jquery-ui.css">
    <link rel="stylesheet" href="css/owl.carousel.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">
    <link rel="stylesheet" href="css/bootstrap-datepicker.css">
    <link rel="stylesheet" href="css/animate.css">
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mediaelement@4.2.7/build/mediaelementplayer.min.css">
    
    <link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">
  
    <link rel="stylesheet" href="css/aos.css">

    <link rel="stylesheet" href="css/style.css">
    
  </head>
  <body>
  
  <div class="site-wrap">

    <div class="site-mobile-menu">
      <div class="site-mobile-menu-header">
        <div class="site-mobile-menu-close mt-3">
          <span class="icon-close2 js-menu-toggle"></span>
        </div>
      </div>
      <div class="site-mobile-menu-body"></div>
    </div> <!-- .site-mobile-menu -->
    
    
    <div class="site-navbar-wrap js-site-navbar bg-white">
      
      <div class="container">
        <div class="site-navbar bg-light">
          <div class="py-1">
            <div class="row align-items-center">
              <div class="col-8">
                <h2 class="mb-0 site-logo"><a href="index.html">CML-IOT 2020</a></h2>
              </div>
              
            </div>
          </div>
        </div>
      </div>
    </div>
  
    
    
      
    <div class="site-blocks-cover overlay" style="background-image: url(images/cancun.jpg);" data-aos="fade" data-stellar-background-ratio="0.5">
      <div class="container">
        <div class="row align-items-center justify-content-center">
          <div class="col-md-10 text-center" data-aos="fade">
            <font size="10" color="white">Continual and Multimodal Learning for Internet of Things</font>
            <p class="mb-5">September 13, 2020 &bullet; Cancun, Mexico</p>
      <p class="mb-6">A UbiComp 2020 Workshop</p>
          </div>
        </div>
      </div>
    </div>  

<a name="more"></a>

      

    <div class="site-section">
      <div class="container">
        
        <div class="row">
          <div class="col-md-12 mx-auto text-left section-heading">
             <h3 class="mb-5 text-uppercase">About CML-IOT </h3> 
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
  <p>Internet of Things (IoT) provides streaming, large-amount, and multimodal sensing data over time. The statistical properties of these data are often significantly different by sensing modalities and temporal traits, which are hardly captured by conventional learning methods. Continual and multimodal learning allows integration, adaptation and generalization of the knowledge learnt from previous experiential data collected with heterogeneity to new situations. Therefore, continual and multimodal learning is an important step to improve the estimation, utilization, and security of real-world data from IoT devices. </p>
  <br />
    <br />
          </div>




          <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Call for Papers </h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
            <p>
        This workshop aims to explore the intersection and combination of continual machine learning and multimodal modeling with applications in Internet of Things. The workshop welcomes works addressing these issues in different applications and domains, such as human-centric sensing, smart cities, health and wellness, privacy and security, etc. We aim at bringing together researchers from different areas to establish a multidisciplinary community and share the latest research. 
        </p>
  <p>We focus on the novel learning methods that can be applied on streaming multimodal data:</p>
  <li>continual learning </li>
  <li>transfer learning </li>
  <li>federated learning </li>
<li>few-shot learning </li>
<li>multi-task learning </li>
<li>reinforcement learning </li>
<li>learning without forgetting </li>
<li>individual and/or institutional privacy </li>
<li>balance on-device and off-device learning </li>
<li>manage high volume data flow</li>

        
   <br />
      
  <p>We also welcome continual learning methods that target:  </p>
<li>data distribution changed caused by the fast-changing dynamic physical environment</li>
<li>missing, imbalanced, or noisy data under multimodal sensing scenarios</li>
      
  <br />
      
<p> Novel applications or interfaces on streaming multimodal data are also related topics. </p>

      <br />
<p>As examples, the data modalities include but not limited to: WiFi, GPS, RFID, vibration, accelerometer, pressure, temperature, humidity, biochemistry, image, video, audio, speech, natural language, virtual reality, etc.</p>
    <br />
    <br />    
          </div>



           <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Important Dates </h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
  <li>Submission deadline: July 10, 2020
  <li>Notification of acceptance: July 24, 2020 </li>
  <li>Deadline for camera ready version: July 31, 2020</li>
  <li>Workshop: September 13, 2020</li>
  <p> </p>
  <p><a href="https://new.precisionconference.com" class="btn btn-primary px-4 py-2">Submit Now</a></p>
  <br />
    <br />
          </div>

           <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Submission Guidelines </h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
            <p>Please submit papers using the <a href="https://www.acm.org/publications/proceedings-template">ACM SIGCHI portrait template</a>. We invite papers of varying length from 2 to 6 pages, plus additional pages for the reference; i.e., the reference page(s) are not counted to the limit of 6 pages. Accepted papers will be included in the ACM Digital Library and supplemental proceedings of the conference. Reviews are not double-blind, and author names and affiliations should be listed.</p>
  <br />
    <br />     
          </div>

      <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Keynote </h3>
            </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">

<h5 class="mb-5 text">Edge-Assisted Collaborative IoT Machine Intelligence, Speaker: Archan Misra, Singapore Management University</h5>

<div class="row justify-content-center">
          <div class="col-12">

<p>Abstract: To support real-time & sustainable machine intelligence that exploits the rapid growth in sensor deployments and wearable devices (e.g., inertial, radar), there is a need to optimize the execution of machine learning (ML) pipelines on resource-constrained embedded devices. Through this talk, I shall introduce the paradigm of collaborative machine intelligence (CMI), where the sensing and ML pipelines on individual wearable and IoT devices collaborate, in real-time. I shall describe why CMI also requires the evolution of an edge node, from merely a resource for offloaded computation, to a platform for coordination among IoT devices. Using an exemplar video surveillance application, I will describe how IoT-based CMI can provide dramatic reductions in sensing energy, while also improving accuracy and minimizing communication overheads. I shall also describe how CMI can be leveraged to support (a) ultra-low power (and even battery-less operation) of wearable devices and (b) resource-efficient instruction comprehension for natural human-machine interfaces.  </p>
  
<p>Speaker Bio: Archan Misra is Vice Provost (Research) and Professor of Computer Science at Singapore Management University (SMU). He is the Director of SMU’s Center for Applied Smart-Nation Analytics (CASA), which is developing pervasive technologies for smart city infrastructure and applications. Archan has led a number of multi-million dollar, large-scale research initiatives at SMU, including the LiveLabs research center, and is a current recipient of the prestigious Investigator grant (from Singapore’s National Research Foundation) for sustainable man-machine interaction intelligence. Over a 20+ year research career spanning both academics and industry (at IBM Research and Bellcore), Archan has published on, and practically deployed, technologies spanning wireless networking, mobile & wearable sensing and urban mobility analytics. His current research interests lie in ultra-low energy execution of machine intelligence algorithms using wearable and IoT devices. Archan holds a Ph.D. from the University of Maryland at College Park, and chaired the IEEE Computer Society's Technical Committee on Computer Communications (TCCC) from 2005-2007. </p>
<br>
<br>
  </div>
        </div>


        </div>
        <div class="row justify-content-center">
    
     <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Organizers</h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
            <h5 class="mb-5 text">Workshop Chairs (Feel free to contact us by cmliot2020@gmail.com, if you have any questions.) </h5>
            <li>Susu Xu (Qualcomm AI Research)</li>
            <li>Tong Yu (Samsung Research America)</li>
            <li>Shijia Pan (UC Merced)</li>
            <br />
            <br />
            <h5 class="mb-5 text">Advising Committee</h5>
            <li>Pei Zhang (Carnegie Mellon University)</li>
            <li>Hae Young Noh (Stanford University)</li>
            <li>Joseph Soriaga (Qualcomm AI Research)</li>
            <li>Steve Gu (AiFi Inc.)</li>
            <li>Hongxia Jin (Samsung Research America)</li>
            <br />
            <br />
      
      <h5 class="mb-5 text">Technical Program Committee</h5>
            <li>Xiaoxi Zhang (Carnegie Mellon University)</li>
            <li>Sheng Li (University of Georgia)</li>
            <li>Xidong Pi (Uber ATG)</li>
            <li>Zhihan Fang (Rutgers University)</li>
            <li>Wei Ma (The Hong Kong Polytechnic University)</li>
            <li>Adeola Bannis (Carnegie Mellon University)</li>
            <li>Mostafa Mirshekari (Stanford University)</li>
            <li>Rui Ma (Tsinghua University)</li>
            <li>Shanghang Zhang (UC Berkeley)</li>
            <li>Aniruddha Basak (Amazon)</li>
            <li>Handong Zhao (Adobe Research)</li>
                        
      <br />
    <br />
  </div>
        <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Agenda (Cancun Time) </h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
            
      <h5 class="mb-5 text">Welcome! (9:00 - 9:15)</h5>  
      <h5 class="mb-5 text">Keynote (9:15 - 10:00), Speaker: Archan Misra, Singapore Management University </h5>
      
      
        <h5 class="mb-6 text">Session 1: Multimodal Sensing and Learning for Smart Health (10:00 - 11:00)</h5>
        <li>RCH: Robust Calibration Based on Historical Data for Low-Cost Air Quality Sensor Deployments, Guodong Li, Rui Ma, Xinyu Liu, Yue Wang, Lin Zhang</li>
        <li>Spinal curve assessment of idiopathic scoliosis with a small dataset via a multi-scale keypoint estimation approach, Liu Tianyu, Yang Yukang, Wang Yu, Sun Ming, Fan Wenhui, Wu Cheng, Cody Bunger </li>
        <li>Digital Twin - A Machine Learning Approach to Predict Individual Stress Levels in Extreme Environments, Dr. Constantin Stefan Scheuermann, Thomas Binderberger, Nadine von Frankenberg, Dr. med Andreas Werner</li> 
      
      <br>
      <h5 class="mb-5 text">Lunch (11:00 - 12:30) </h5>
  
      <h5 class="mb-6 text">Session 2: Continual and Multimodal Learning for Smart Building (12:30 - 13:30)</h5> 
      <li>Space Utilization and Activity Recognition using 3D Stereo Vision Camera inside an Educational Building, Anooshmita Das Das, Krister, Mikkel Baun Kjærgaard</li> 
      <li>Fine-grained Activities Recognition with Coarse-grained Labeled Multi-modal Data, Zhizhang Hu, Tong Yu, Yue Zhang, Shijia Pan</li> 
      <li>Accurate Trajectory Prediction in a Smart Building using Recurrent Neural Networks, Anooshmita Das, Mikkel Baun Kjærgaard</li> 
      
      <br>

       <h5 class="mb-6 text">Session 3: System Evaluation and Calibration (13:30 - 13:50) </h6> 
      <li>Evaluation of Federated Learning Aggregation Algorithms, Sannara Ek, François Portet, Philippe Lalanda and German Vega</li>   
      <br>

      <h5 class="mb-5 text">Registration/Doors Close (14:00) </h5>  
            
      <br>
      Note: For each paper presentation, there are 15 minutes for presentation and 5 minutes for Q&A.
    
          </div>
        </div>       
      </div>
    </div>

    
    

          
          
        <div class="row pt-5 mt-5 text-center">
          <div class="col-md-12">
            <p>
            <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
            Copyright &copy; <script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script>document.write(new Date().getFullYear());</script> All Rights Reserved | This template is made with <i class="icon-heart text-primary" aria-hidden="true"></i> by <a href="https://colorlib.com" target="_blank" >Colorlib</a>
            <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
            </p>
          </div>
          
        </div>
      </div>
    </footer>
  </div>

  <script src="js/jquery-3.3.1.min.js"></script>
  <script src="js/jquery-migrate-3.0.1.min.js"></script>
  <script src="js/jquery-ui.js"></script>
  <script src="js/popper.min.js"></script>
  <script src="js/bootstrap.min.js"></script>
  <script src="js/owl.carousel.min.js"></script>
  <script src="js/jquery.stellar.min.js"></script>
  <script src="js/jquery.countdown.min.js"></script>
  <script src="js/jquery.magnific-popup.min.js"></script>
  <script src="js/bootstrap-datepicker.min.js"></script>
  <script src="js/aos.js"></script>

  
  <script src="js/mediaelement-and-player.min.js"></script>

  <script src="js/main.js"></script>
    

  <script>
      document.addEventListener('DOMContentLoaded', function() {
                var mediaElements = document.querySelectorAll('video, audio'), total = mediaElements.length;

                for (var i = 0; i < total; i++) {
                    new MediaElementPlayer(mediaElements[i], {
                        pluginPath: 'https://cdn.jsdelivr.net/npm/mediaelement@4.2.7/build/',
                        shimScriptAccess: 'always',
                        success: function () {
                            var target = document.body.querySelectorAll('.player'), targetTotal = target.length;
                            for (var j = 0; j < targetTotal; j++) {
                                target[j].style.visibility = 'visible';
                            }
                  }
                });
                }
            });
    </script>

  </body>
</html>

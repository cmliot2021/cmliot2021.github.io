<!DOCTYPE html>
<html lang="en">
  <head>
    <title>CML-IOT 2021</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <link href="https://fonts.googleapis.com/css?family=Amatic+SC:400,700|Work+Sans:300,400,700" rel="stylesheet">
    <link rel="stylesheet" href="fonts/icomoon/style.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/magnific-popup.css">
    <link rel="stylesheet" href="css/jquery-ui.css">
    <link rel="stylesheet" href="css/owl.carousel.min.css">
    <link rel="stylesheet" href="css/owl.theme.default.min.css">
    <link rel="stylesheet" href="css/bootstrap-datepicker.css">
    <link rel="stylesheet" href="css/animate.css">
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mediaelement@4.2.7/build/mediaelementplayer.min.css">
    
    <link rel="stylesheet" href="fonts/flaticon/font/flaticon.css">
  
    <link rel="stylesheet" href="css/aos.css">

    <link rel="stylesheet" href="css/style.css">
    
  </head>
  <body>
  
  <div class="site-wrap">

    <div class="site-mobile-menu">
      <div class="site-mobile-menu-header">
        <div class="site-mobile-menu-close mt-3">
          <span class="icon-close2 js-menu-toggle"></span>
        </div>
      </div>
      <div class="site-mobile-menu-body"></div>
    </div> <!-- .site-mobile-menu -->
    
    
    <div class="site-navbar-wrap js-site-navbar bg-white">
      
      <div class="container">
        <div class="site-navbar bg-light">
          <div class="py-1">
            <div class="row align-items-center">
              <div class="col-8">
                <h2 class="mb-0 site-logo"><a href="index.html">CML-IOT 2021</a></h2>
              </div>
              
            </div>
          </div>
        </div>
      </div>
    </div>
  
    
    
      
    <div class="site-blocks-cover overlay" style="background-image: url(images/montreal.jpg);" data-aos="fade" data-stellar-background-ratio="0.5">
      <div class="container">
        <div class="row align-items-center justify-content-center">
          <div class="col-md-10 text-center" data-aos="fade">
            <font size="8" color="white">3rd Workshop on Continual and Multimodal Learning for Internet of Things</font>
            <p class="mb-5"> August 21, 2021 &bullet; Online </p>
      <p class="mb-6">Co-located with IJCAI 2021</p>
          </div>
        </div>
      </div>
    </div>  

<a name="more"></a>

      

    <div class="site-section">
      <div class="container">
        
        <div class="row">
          <div class="col-md-12 mx-auto text-left section-heading">
             <h3 class="mb-5 text-uppercase">About CML-IOT (previous editions: <a href="https://cmliot2020.github.io/"> CML-IOT'20 </a>, <a href="https://cmliot2019.github.io/"> CML-IOT'19 </a>) </h3> 
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
  <p>Internet of Things (IoT) provides streaming, large-amount, and multimodal data (e.g., natural language, speech, image, video, audio, virtual reality, WiFi, GPS, RFID, vibration) over time. The statistical properties of these data are often significantly different by sensing modalities and temporal traits, which are hardly captured by conventional learning methods. Continual and multimodal learning allows integration, adaptation and generalization of the knowledge learnt from previous experiential data collected with heterogeneity to new situations. Therefore, continual and multimodal learning is an important step to improve the estimation, utilization, and security of real-world data from IoT devices. </p>
  <br />
    <br />
          </div>




          <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Call for Papers </h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
            <p>
        This workshop aims to explore the intersection and combination of continual machine learning and multimodal modeling with applications in Internet of Things. The workshop welcomes works addressing these issues in different applications and domains, such as natural language processing, computer vision, human-centric sensing, smart cities, health, etc. We aim at bringing together researchers from different areas to establish a multidisciplinary community and share the latest research. 
        </p>
  <p>We focus on the novel learning methods that can be applied on streaming multimodal data:</p>
  <li>continual learning </li>
  <li>transfer learning </li>
  <li>federated learning </li>
<li>few-shot learning </li>
<li>multi-task learning </li>
<li>reinforcement learning </li>
<li>learning without forgetting </li>
<li>individual and/or institutional privacy </li>
<li>manage high volume data flow</li>

        
   <br />
      
  <p>We also welcome continual learning methods that target:  </p>
<li>data distribution changed caused by the fast-changing dynamic physical environment</li>
<li>missing, imbalanced, or noisy data under multimodal data scenarios</li>
      
  <br />
      
<p> Novel applications or interfaces on streaming multimodal data are also related topics. </p>

      <br />
<p>As examples, the data modalities include but not limited to: natural language, speech, image, video, audio, virtual reality, biochemistry, WiFi, GPS, RFID, vibration, accelerometer, pressure, temperature, humidity, etc.</p>
    <br />
    <br />    
          </div>



           <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Important Dates </h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
  <li>Submission deadline: May 12, 2021
  <li>Notification of acceptance: May 29, 2021 </li>
  <li>Deadline for camera ready version: June 12, 2021</li>
  <li>Workshop: August 21, 2021</li>
  <p> </p>
  <p><a href="https://cmt3.research.microsoft.com/CMLIOT2021" class="btn btn-primary px-4 py-2">Submit Now</a></p>
  <br />
    <br />
          </div>

           <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Submission Guidelines </h3>
          </div>
          </div>
          <div class="row justify-content-center">
          <div class="col-12">
            <p>Please submit papers using the <a href="https://www.ijcai.org/authors_kit"> the IJCAI author kit</a>. We invite papers of varying length from 2 to 6 pages, plus additional pages for the reference; i.e., the reference page(s) are not counted to the limit of 6 pages. The reviewing process is double-blind. The qualified accepted papers will be invited to be extended for a journal submission at <a href="https://www.frontiersin.org/research-topics/21706/continual-and-multimodal-learning-for-internet-of-things"> Frontiers in Big Data</a>. </p>
  <br/>
    <br/>     
          </div>
            
            

      

      <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Invited Keynote Speakers</h3>
            </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
		  <h5 class="mb-5 text">Keynote 1: Knowledge-Guided Graph Representation Learning, Speaker: Sheng Li, University of Georgia</h5>	  
<div class="row justify-content-center">
          <div class="col-12">  
	<p>Abstract: Graph-structured data are ubiquitous, which have been extensively used in many real-world applications. In this talk, I will present our recent work on graph representation learning with applications in multiple domains. First, we leverage the line graph theory and propose novel graph neural networks, which jointly learn embeddings for both nodes and edges. Second, we investigate how to incorporate commonsense and domain knowledge to graph representation learning, and present several applications in computer vision, natural language processing, and recommender systems. Finally, future work on knowledge-guided graph representation learning will also be discussed.
</p>
	<p> Bio: Dr. Sheng Li is an Assistant Professor of Computer Science at the University of Georgia (UGA). Before joining UGA in 2018, he was a Data Scientist at Adobe Research. He obtained his Ph.D. degree in computer engineering from Northeastern University in 2017. Dr. Li's research interests include graph-based machine learning, visual intelligence, user modeling, causal inference, and trustworthy artificial intelligence. He has published over 100 papers at peer-reviewed conferences and journals, and has received over 10 research awards, such as the INNS Young Investigator Award, M. G. Michael Award, Adobe Data Science Research Award, Cisco Faculty Award, and SIAM SDM Best Paper Award. He has served as Associate Editor of seven international journals such as IEEE Transactions on Circuits and Systems for Video Technology and IEEE Computational Intelligence Magazine, as an Area Chair of ICLR and ICPR, and as a Senior Program Committee member of AAAI and IJCAI. He is a senior member of IEEE. </p>

		  </div>
        </div>
		  
		  <br />
            <br />
		  
<div class="row justify-content-center">
          <div class="col-12">
		  
	<h5 class="mb-5 text">Keynote 2: Large-scale Vision-and-Language Pre-training for Multimodal Learning, Speaker: Zhe Gan, Microsoft</h5>	  
<div class="row justify-content-center">
          <div class="col-12">  
	
	<p>Abstract: With the advent of models such as OpenAI CLIP and DALL-E, transformer-based vision-and-language pre-training has become an increasingly hot research topic. In this talk, I will share some of our recent work in this direction and try to answer the following questions. First, how to perform vision-and-language pre-training? Second, how to enhance the performance of pre-trained models via adversarial training? Third, how robust are these pre-trained models? And finally, how can we extend image-text pre-training to video-text pre-training? Accordingly, I will present UNITER, VILLA, Adversarial VQA, HERO, and ClipBERT to answer these questions. At last, I will also briefly discuss the challenges and future directions for vision-and-language pre-training. </p>
	<p>Bio: Dr. Zhe Gan is a Principal Researcher at Microsoft. He received the PhD degree from Duke University in 2018. Before that, he received the Master’s and Bachelor’s degree from Peking University in 2013 and 2010, respectively. His current research interests include vision-and-language representation learning, self-supervised pre-training, and adversarial machine learning. He received the Best Student Paper Honorable Mention Award at CVPR 2021 and WACV 2021, and Outstanding Senior Program Committee Member Award at AAAI 2020. He has been regularly serving as an Area Chair for NeurIPS, ICML, ICLR, ACL, and AAAI. </p>

  </div>
        </div>


        </div>
        <div class="row justify-content-center">
    
     <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Organizers</h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
            <h5 class="mb-5 text">Workshop Chairs (Feel free to contact us by cmliot2021@gmail.com, if you have any questions.) </h5>       
            <li>Tong Yu (Adobe Research)</li>
            <li>Susu Xu (Stony Brook University)</li>
            <li>Handong Zhao (Adobe Research) </li>
            <li>Ruiyi Zhang (Adobe Research) </li>
            <li>Shijia Pan (UC Merced)</li>
            <br />
            <br />
            <h5 class="mb-5 text">Advising Committee </h5>
            <li>Nicholas Lane (University of Cambridge and Samsung AI)</li>
            <li>Jennifer Healey (Adobe Research) </li>
            <li> Branislav Kveton (Google Research) </li>
            <li> Zheng Wen (DeepMind) </li>
            <li> Changyou Chen (University at Buffalo) </li>
            <br />
            <br />
      
      <h5 class="mb-5 text">Technical Program Committee </h5>
    <li>Bang An (University of Maryland)</li>
    <li>Guan-Lin Chao (Carnegie Mellon University)</li>
 <li>Jonathon Fagert (Baldwin Wallace University)</li>
 <li>Gao Tang (University of Illinois at Urbana-Champaign) </li>
 <li>Ajinkya Kale (Adobe)</li>
 <li>Chuanyi Li (Nanjing University)</li>
 <li>Kunpeng Li (Northeastern University)</li>
 <li>Wei Ma (Hongkong Polytech University)</li>
 <li>Mostafa Mirshekari (Searchable.ai)</li>
 <li>Xidong Pi (Aurora)</li>
 <li>Can Qin (Northeastern University)</li>
 <li>Shijing Si (Pingan Technology AI Center)</li>
 <li>Rui Wang (Duke University)</li>
 <li>Yikun Xian (Rutgers University)</li>
  <li>Yifan Zhou (University at Buffalo)</li> 
  <li>Ming Zeng (Facebook)</li>
      <br />
    <br />
  </div>
        <div class="col-md-12 mx-auto text-left section-heading">
            <h3 class="mb-5 text-uppercase">Agenda (Montreal time) </h3>
          </div>
        </div>
        <div class="row justify-content-center">
          <div class="col-12">
           
	    <h5 class="mb-5 text">Welcome! (10:00 - 10:15)</h5>  
	 	
	    <h5 class="mb-5 text">Keynote 1 (10:15 - 11:00), Speaker: Sheng Li, University of Georgia </h5>  
	    
	    <h5 class="mb-6 text">Session 1: Privacy Perserving Continual and Multimodal Learning (11:00 - 12:00)  </h6> 
	    <li> A distillation-based approach integrating continual learning and federated learning for pervasive services, Anastasiia USMANOVA, Philippe Lalanda, Francois Portet, German Vega </li> 

	    <li> Capturing occupant activities of daily living through sensor fusion: framework, modelling, privacy aspects and applications, Anooshmita Das, Masab Khalid Annaqeeb </li> 

	    <li> Continual Distributed Learning for Crisis Management, Aman Priyanshu, Mudit Sinha, Shreyans Mehta</li> 
	    
	    <br>
	    <h5 class="mb-5 text"> Lunch Break (12:00 - 13:00) </h5>
		  
	    <h5 class="mb-5 text">  Keynote 2 (13:00 - 13:45), Speaker:  Zhe Gan, Microsoft </h5>
		  
        <h5 class="mb-6 text">Session 2: Data Augmentation for Continual and Multimodal Learning (13:45 - 14:45)  </h5>
        <li> SDA: Improving Text Generation with Self Data Augmentation, Ping Yu, Ruiyi Zhang, Yang Zhao, Changyou Chen </li> 

	    <li> Adopting Active Learning for User Requests Classification, Yuan Zhang, Chuanyi Li, Bin Luo </li> 

	    <li> Fully Unsupervised Domain Adaptation, Zhimeng Yang, Yazhou Ren, Zirui Wu, Ming Zeng, Jie Xu </li> 


	    
	    <br>
	
		

	    <h5 class="mb-6 text"> Session 3: Novel Application (14:45 - 15:00) </h5> 
	    <li> PopCTR: A Multi-Modal Architecture for Click-Through Rate Prediction of Bank Pop-up Advertisements, Liqiang Song, Jiahao Yang, Chengtian Ren, Mengqiu Yao, Yan Yi, Ye Bi, Jianming Wang, Jing Xiao, Ming Yan, Baijun Shen </li> 
	    



	    <br>

	    <h5 class="mb-5 text">Summary (15:00 - 15:10) </h5>  
            
            <br>
	    Note: For each paper presentation, there are 15 minutes for presentation and 5 minutes for Q&A.
		
          </div>
        </div>
      </div>
    </div>

    
    

          
          
        <div class="row pt-5 mt-5 text-center">
          <div class="col-md-12">
            <p>
            <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
            Copyright &copy; <script data-cfasync="false" src="/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js"></script><script>document.write(new Date().getFullYear());</script> All Rights Reserved | This template is made with <i class="icon-heart text-primary" aria-hidden="true"></i> by <a href="https://colorlib.com" target="_blank" >Colorlib</a>
            <!-- Link back to Colorlib can't be removed. Template is licensed under CC BY 3.0. -->
            </p>
          </div>
          
        </div>
      </div>
    </footer>
  </div>

  <script src="js/jquery-3.3.1.min.js"></script>
  <script src="js/jquery-migrate-3.0.1.min.js"></script>
  <script src="js/jquery-ui.js"></script>
  <script src="js/popper.min.js"></script>
  <script src="js/bootstrap.min.js"></script>
  <script src="js/owl.carousel.min.js"></script>
  <script src="js/jquery.stellar.min.js"></script>
  <script src="js/jquery.countdown.min.js"></script>
  <script src="js/jquery.magnific-popup.min.js"></script>
  <script src="js/bootstrap-datepicker.min.js"></script>
  <script src="js/aos.js"></script>

  
  <script src="js/mediaelement-and-player.min.js"></script>

  <script src="js/main.js"></script>
    

  <script>
      document.addEventListener('DOMContentLoaded', function() {
                var mediaElements = document.querySelectorAll('video, audio'), total = mediaElements.length;

                for (var i = 0; i < total; i++) {
                    new MediaElementPlayer(mediaElements[i], {
                        pluginPath: 'https://cdn.jsdelivr.net/npm/mediaelement@4.2.7/build/',
                        shimScriptAccess: 'always',
                        success: function () {
                            var target = document.body.querySelectorAll('.player'), targetTotal = target.length;
                            for (var j = 0; j < targetTotal; j++) {
                                target[j].style.visibility = 'visible';
                            }
                  }
                });
                }
            });
    </script>

  </body>
</html>
